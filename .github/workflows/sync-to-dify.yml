name: Sync Articles to Dify RAG

on:
  push:
    branches: [main2]
    paths:
      - 'articles/**.md'
      - 'articles/**.mdx'
  pull_request:
    types: [closed]
    branches: [main2]
    paths:
      - 'articles/**'
  workflow_dispatch:
    inputs:
      sync_all:
        description: 'Sync all articles'
        required: false
        type: boolean
        default: false

jobs:
  sync-articles:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 2
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: pip install requests pyyaml
    
    - name: Create sync script
      run: |
        cat > sync_dify.py << 'SCRIPT'
        import os
        import sys
        import json
        import requests
        import hashlib
        import time
        from pathlib import Path
        
        class DifyRAGSync:
            def __init__(self):
                self.api_key = os.environ['DIFY_API_KEY']
                self.api_url = os.environ['DIFY_API_URL']
                self.dataset_id = os.environ['DIFY_DATASET_ID']
                self.headers = {
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                }
                self.sync_all = os.environ.get('SYNC_ALL', 'false').lower() == 'true'
                # articles„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÊåáÂÆö
                self.articles_dir = 'articles'
                
            def get_existing_documents(self):
                """Êó¢Â≠ò„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ‰∏ÄË¶ß„ÇíÂèñÂæó"""
                docs = {}
                page = 1
                while True:
                    url = f"{self.api_url}/datasets/{self.dataset_id}/documents?page={page}&limit=100"
                    response = requests.get(url, headers=self.headers)
                    if response.status_code != 200:
                        break
                    data = response.json()
                    for doc in data.get('data', []):
                        docs[doc['name']] = doc['id']
                    if not data.get('has_more', False):
                        break
                    page += 1
                return docs
            
            def get_changed_files(self):
                """articles„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÂ§âÊõ¥„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó"""
                if self.sync_all:
                    # articles„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÂÖ®„Éï„Ç°„Ç§„É´„ÇíÂêåÊúü
                    articles_path = Path(self.articles_dir)
                    if not articles_path.exists():
                        print(f"‚ö†Ô∏è Directory '{self.articles_dir}' not found")
                        return []
                    
                    files = []
                    for ext in ['*.md', '*.mdx']:
                        files.extend(articles_path.glob(ext))
                        # „Çµ„Éñ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇÇÂê´„ÇÅ„ÇãÂ†¥Âêà
                        files.extend(articles_path.rglob(ext))
                    
                    return [str(f) for f in files]
                else:
                    # Â§âÊõ¥„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÅÆ„ÅøÔºàarticles„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖÔºâ
                    import subprocess
                    result = subprocess.run(
                        ['git', 'diff', '--name-only', 'HEAD^', 'HEAD'],
                        capture_output=True, text=True
                    )
                    
                    if result.returncode != 0:
                        # fallback: articles„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÊúÄËøë„ÅÆmd„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó
                        articles_path = Path(self.articles_dir)
                        if articles_path.exists():
                            files = list(articles_path.glob('*.md'))[:5]
                            return [str(f) for f in files]
                        return []
                    
                    files = result.stdout.strip().split('\n')
                    # articles„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆ„Åø„Éï„Ç£„É´„Çø
                    return [
                        f for f in files 
                        if f.startswith(self.articles_dir + '/') and f.endswith(('.md', '.mdx'))
                    ]
            
            def extract_zenn_metadata(self, content):
                """ZennÂΩ¢Âºè„ÅÆ„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫"""
                metadata = {}
                if content.startswith('---'):
                    try:
                        end = content.index('---', 3)
                        frontmatter = content[3:end].strip()
                        for line in frontmatter.split('\n'):
                            if ':' in line:
                                key, value = line.split(':', 1)
                                key = key.strip()
                                value = value.strip().strip('"\'')
                                
                                # ZennÁâπÊúâ„ÅÆ„É°„Çø„Éá„Éº„Çø
                                if key in ['title', 'emoji', 'type', 'topics', 'published']:
                                    metadata[key] = value
                                    
                        # topics„Çí„É™„Çπ„Éà„Å´Â§âÊèõ
                        if 'topics' in metadata:
                            topics_str = metadata['topics'].strip('[]')
                            metadata['topics'] = [t.strip().strip('"') for t in topics_str.split(',')]
                    except:
                        pass
                return metadata
            
            def sync_document(self, filepath, existing_docs):
                """„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíÂêåÊúü"""
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if not content.strip():
                        return False
                    
                    # „Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÊã°ÂºµÂ≠ê„ÇíÈô§Âéª
                    doc_name = os.path.basename(filepath)
                    
                    # Zenn„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫
                    zenn_metadata = self.extract_zenn_metadata(content)
                    
                    # „É°„Çø„Éá„Éº„Çø„ÇíÊßãÁØâ
                    metadata = {
                        "source": "github_articles",
                        "path": filepath,
                        "repository": os.environ.get('GITHUB_REPOSITORY', ''),
                        "commit": os.environ.get('GITHUB_SHA', '')[:7],
                        "sync_time": time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime()),
                        "articles_dir": self.articles_dir
                    }
                    
                    # Zenn„É°„Çø„Éá„Éº„Çø„ÇíËøΩÂä†
                    metadata.update(zenn_metadata)
                    
                    # „Ç´„ÉÜ„Ç¥„É™„ÇíÊé®ÂÆöÔºà„Éï„Ç°„Ç§„É´Âêç„ÇÑ„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„Åã„ÇâÔºâ
                    path_parts = filepath.split('/')
                    if len(path_parts) > 2:  # articles/subdirectory/file.md
                        metadata['category'] = path_parts[1]
                    
                    payload = {
                        "name": doc_name,
                        "text": content,
                        "indexing_technique": "high_quality",
                        "process_rule": {
                            "mode": "automatic",
                            "rules": {
                                "pre_processing_rules": [
                                    {"id": "remove_extra_spaces", "enabled": True},
                                    {"id": "remove_urls_emails", "enabled": False}
                                ],
                                "segmentation": {
                                    "separator": "\n\n",
                                    "max_tokens": 1000
                                }
                            }
                        },
                        "metadata": metadata
                    }
                    
                    if doc_name in existing_docs:
                        # Êõ¥Êñ∞
                        url = f"{self.api_url}/datasets/{self.dataset_id}/documents/{existing_docs[doc_name]}/update_by_text"
                        response = requests.post(url, headers=self.headers, json=payload)
                        action = "Updated"
                    else:
                        # Êñ∞Ë¶è‰ΩúÊàê
                        url = f"{self.api_url}/datasets/{self.dataset_id}/document/create_by_text"
                        response = requests.post(url, headers=self.headers, json=payload)
                        action = "Created"
                    
                    if response.status_code in [200, 201]:
                        print(f"‚úÖ {action}: {filepath}")
                        if zenn_metadata.get('title'):
                            print(f"   üìù Title: {zenn_metadata['title']}")
                        if zenn_metadata.get('topics'):
                            print(f"   üè∑Ô∏è Topics: {', '.join(zenn_metadata['topics'])}")
                        return True
                    else:
                        print(f"‚ùå Failed to {action.lower()}: {filepath}")
                        print(f"   Response: {response.status_code} - {response.text[:200]}")
                        return False
                        
                except Exception as e:
                    print(f"‚ùå Error processing {filepath}: {str(e)}")
                    return False
            
            def run(self):
                """„É°„Ç§„É≥Âá¶ÁêÜ"""
                print("üöÄ Starting Dify RAG synchronization for /articles directory...")
                print(f"   API URL: {self.api_url}")
                print(f"   Dataset ID: {self.dataset_id}")
                print(f"   Articles Directory: {self.articles_dir}")
                
                # articles„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÂ≠òÂú®Á¢∫Ë™ç
                if not Path(self.articles_dir).exists():
                    print(f"‚ùå Error: '{self.articles_dir}' directory not found!")
                    print("   Please ensure the articles directory exists in your repository.")
                    sys.exit(1)
                
                # Êó¢Â≠ò„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíÂèñÂæó
                existing_docs = self.get_existing_documents()
                print(f"üìö Found {len(existing_docs)} existing documents in Dify")
                
                # Â§âÊõ¥„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó
                files = self.get_changed_files()
                if not files:
                    print("‚ÑπÔ∏è No articles to sync")
                    return
                
                print(f"üìù Found {len(files)} articles to process...")
                for f in files[:5]:  # ÊúÄÂàù„ÅÆ5„Éï„Ç°„Ç§„É´„ÇíË°®Á§∫
                    print(f"   - {f}")
                if len(files) > 5:
                    print(f"   ... and {len(files) - 5} more")
                
                # ÂêåÊúüÂÆüË°å
                success = 0
                failed = 0
                for filepath in files:
                    if self.sync_document(filepath, existing_docs):
                        success += 1
                    else:
                        failed += 1
                
                # „Çµ„Éû„É™„Éº
                print(f"\nüìä Sync Summary:")
                print(f"   ‚úÖ Success: {success}")
                print(f"   ‚ùå Failed: {failed}")
                print(f"   üìÅ Total: {len(files)}")
                
                # GitHub Actions „Çµ„Éû„É™„Éº
                summary_file = os.environ.get('GITHUB_STEP_SUMMARY')
                if summary_file:
                    with open(summary_file, 'w') as f:
                        f.write(f"# üìö Dify Articles Sync Results\n\n")
                        f.write(f"| Metric | Count |\n")
                        f.write(f"|--------|-------|\n")
                        f.write(f"| ‚úÖ Success | {success} |\n")
                        f.write(f"| ‚ùå Failed | {failed} |\n")
                        f.write(f"| üìÅ Total Files | {len(files)} |\n")
                        f.write(f"| üìö Existing Docs | {len(existing_docs)} |\n")
                        f.write(f"| üìÇ Directory | `{self.articles_dir}` |\n")
                        f.write(f"\n**Dataset:** `{self.dataset_id}`\n")
                        f.write(f"**Commit:** `{os.environ.get('GITHUB_SHA', 'unknown')[:7]}`\n")
                        f.write(f"**Repository:** `{os.environ.get('GITHUB_REPOSITORY', 'unknown')}`\n")
                        
                        if files and success > 0:
                            f.write(f"\n### Successfully Synced Articles\n")
                            for filepath in files[:10]:
                                f.write(f"- `{filepath}`\n")
                            if len(files) > 10:
                                f.write(f"- ... and {len(files) - 10} more\n")
                
                if failed > 0:
                    sys.exit(1)
        
        if __name__ == "__main__":
            syncer = DifyRAGSync()
            syncer.run()
        SCRIPT
    
    - name: Run synchronization
      env:
        DIFY_API_KEY: ${{ secrets.DIFY_API_KEY }}
        DIFY_API_URL: ${{ secrets.DIFY_API_URL }}
        DIFY_DATASET_ID: ${{ secrets.DIFY_DATASET_ID }}
        SYNC_ALL: ${{ github.event.inputs.sync_all || 'false' }}
      run: python sync_dify.py
