name: Sync Articles to Dify RAG

on:
  push:
    branches: [main2]
    paths:
      - 'articles/**.md'
      - 'articles/**.mdx'
  pull_request:
    types: [closed]
    branches: [main2]
    paths:
      - 'articles/**'
  workflow_dispatch:
    inputs:
      sync_all:
        description: 'Sync all articles'
        required: false
        type: boolean
        default: false

jobs:
  sync-articles:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 2
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: pip install requests pyyaml
    
    - name: Create sync script
      run: |
        cat > sync_dify.py << 'SCRIPT'
        import os
        import sys
        import json
        import requests
        import hashlib
        import time
        from pathlib import Path
        
        class DifyRAGSync:
            def __init__(self):
                self.api_key = os.environ['DIFY_API_KEY']
                self.api_url = os.environ['DIFY_API_URL']
                self.dataset_id = os.environ['DIFY_DATASET_ID']
                self.headers = {
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                }
                self.sync_all = os.environ.get('SYNC_ALL', 'false').lower() == 'true'
                # articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
                self.articles_dir = 'articles'
                
            def get_existing_documents(self):
                """æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€è¦§ã‚’å–å¾—"""
                docs = {}
                page = 1
                while True:
                    url = f"{self.api_url}/datasets/{self.dataset_id}/documents?page={page}&limit=100"
                    response = requests.get(url, headers=self.headers)
                    if response.status_code != 200:
                        break
                    data = response.json()
                    for doc in data.get('data', []):
                        docs[doc['name']] = doc['id']
                    if not data.get('has_more', False):
                        break
                    page += 1
                return docs
            
            def get_changed_files(self):
                """articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
                if self.sync_all:
                    # articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæœŸ
                    articles_path = Path(self.articles_dir)
                    if not articles_path.exists():
                        print(f"âš ï¸ Directory '{self.articles_dir}' not found")
                        return []
                    
                    files = []
                    for ext in ['*.md', '*.mdx']:
                        files.extend(articles_path.glob(ext))
                        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚å«ã‚ã‚‹å ´åˆ
                        files.extend(articles_path.rglob(ext))
                    
                    return [str(f) for f in files]
                else:
                    # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼ˆarticlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
                    import subprocess
                    result = subprocess.run(
                        ['git', 'diff', '--name-only', 'HEAD^', 'HEAD'],
                        capture_output=True, text=True
                    )
                    
                    if result.returncode != 0:
                        # fallback: articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æœ€è¿‘ã®mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
                        articles_path = Path(self.articles_dir)
                        if articles_path.exists():
                            files = list(articles_path.glob('*.md'))[:5]
                            return [str(f) for f in files]
                        return []
                    
                    files = result.stdout.strip().split('\n')
                    # articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
                    return [
                        f for f in files 
                        if f.startswith(self.articles_dir + '/') and f.endswith(('.md', '.mdx'))
                    ]
            
            def extract_zenn_metadata(self, content):
                """Zennå½¢å¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
                metadata = {}
                if content.startswith('---'):
                    try:
                        end = content.index('---', 3)
                        frontmatter = content[3:end].strip()
                        for line in frontmatter.split('\n'):
                            if ':' in line:
                                key, value = line.split(':', 1)
                                key = key.strip()
                                value = value.strip().strip('"\'')
                                
                                # Zennç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                                if key in ['title', 'emoji', 'type', 'topics', 'published']:
                                    metadata[key] = value
                                    
                        # topicsã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                        if 'topics' in metadata:
                            topics_str = metadata['topics'].strip('[]')
                            metadata['topics'] = [t.strip().strip('"') for t in topics_str.split(',')]
                    except:
                        pass
                return metadata
            
            def sync_document(self, filepath, existing_docs):
                """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŒæœŸ"""
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if not content.strip():
                        return False
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤åŽ»
                    doc_name = os.path.basename(filepath)
                    
                    # Zennãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    zenn_metadata = self.extract_zenn_metadata(content)
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                    metadata = {
                        "source": "github_articles",
                        "path": filepath,
                        "repository": os.environ.get('GITHUB_REPOSITORY', ''),
                        "commit": os.environ.get('GITHUB_SHA', '')[:7],
                        "sync_time": time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime()),
                        "articles_dir": self.articles_dir
                    }
                    
                    # Zennãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    metadata.update(zenn_metadata)
                    
                    # ã‚«ãƒ†ã‚´ãƒªã‚’æŽ¨å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰ï¼‰
                    path_parts = filepath.split('/')
                    if len(path_parts) > 2:  # articles/subdirectory/file.md
                        metadata['category'] = path_parts[1]
                    
                    payload = {
                        "name": doc_name,
                        "text": content,
                        "indexing_technique": "high_quality",
                        "process_rule": {
                            "mode": "automatic",
                            "rules": {
                                "pre_processing_rules": [
                                    {"id": "remove_extra_spaces", "enabled": True},
                                    {"id": "remove_urls_emails", "enabled": False}
                                ],
                                "segmentation": {
                                    "separator": "\n\n",
                                    "max_tokens": 1000
                                }
                            }
                        },
                        "metadata": metadata
                    }
                    
                    if doc_name in existing_docs:
                        # æ›´æ–°
                        url = f"{self.api_url}/datasets/{self.dataset_id}/documents/{existing_docs[doc_name]}/update_by_text"
                        response = requests.post(url, headers=self.headers, json=payload)
                        action = "Updated"
                    else:
                        # æ–°è¦ä½œæˆ
                        url = f"{self.api_url}/datasets/{self.dataset_id}/document/create_by_text"
                        response = requests.post(url, headers=self.headers, json=payload)
                        action = "Created"
                    
                    if response.status_code in [200, 201]:
                        print(f"âœ… {action}: {filepath}")
                        if zenn_metadata.get('title'):
                            print(f"   ðŸ“ Title: {zenn_metadata['title']}")
                        if zenn_metadata.get('topics'):
                            print(f"   ðŸ·ï¸ Topics: {', '.join(zenn_metadata['topics'])}")
                        return True
                    else:
                        print(f"âŒ Failed to {action.lower()}: {filepath}")
                        print(f"   Response: {response.status_code} - {response.text[:200]}")
                        return False
                        
                except Exception as e:
                    print(f"âŒ Error processing {filepath}: {str(e)}")
                    return False
            
            def run(self):
                """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
                print("ðŸš€ Starting Dify RAG synchronization for /articles directory...")
                print(f"   API URL: {self.api_url}")
                print(f"   Dataset ID: {self.dataset_id}")
                print(f"   Articles Directory: {self.articles_dir}")
                
                # articlesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
                if not Path(self.articles_dir).exists():
                    print(f"âŒ Error: '{self.articles_dir}' directory not found!")
                    print("   Please ensure the articles directory exists in your repository.")
                    sys.exit(1)
                
                # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                existing_docs = self.get_existing_documents()
                print(f"ðŸ“š Found {len(existing_docs)} existing documents in Dify")
                
                # å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
                files = self.get_changed_files()
                if not files:
                    print("â„¹ï¸ No articles to sync")
                    return
                
                print(f"ðŸ“ Found {len(files)} articles to process...")
                for f in files[:5]:  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
                    print(f"   - {f}")
                if len(files) > 5:
                    print(f"   ... and {len(files) - 5} more")
                
                # åŒæœŸå®Ÿè¡Œ
                success = 0
                failed = 0
                for filepath in files:
                    if self.sync_document(filepath, existing_docs):
                        success += 1
                    else:
                        failed += 1
                
                # ã‚µãƒžãƒªãƒ¼
                print(f"\nðŸ“Š Sync Summary:")
                print(f"   âœ… Success: {success}")
                print(f"   âŒ Failed: {failed}")
                print(f"   ðŸ“ Total: {len(files)}")
                
                # GitHub Actions ã‚µãƒžãƒªãƒ¼
                summary_file = os.environ.get('GITHUB_STEP_SUMMARY')
                if summary_file:
                    with open(summary_file, 'w') as f:
                        f.write(f"# ðŸ“š Dify Articles Sync Results\n\n")
                        f.write(f"| Metric | Count |\n")
                        f.write(f"|--------|-------|\n")
                        f.write(f"| âœ… Success | {success} |\n")
                        f.write(f"| âŒ Failed | {failed} |\n")
                        f.write(f"| ðŸ“ Total Files | {len(files)} |\n")
                        f.write(f"| ðŸ“š Existing Docs | {len(existing_docs)} |\n")
                        f.write(f"| ðŸ“‚ Directory | `{self.articles_dir}` |\n")
                        f.write(f"\n**Dataset:** `{self.dataset_id}`\n")
                        f.write(f"**Commit:** `{os.environ.get('GITHUB_SHA', 'unknown')[:7]}`\n")
                        f.write(f"**Repository:** `{os.environ.get('GITHUB_REPOSITORY', 'unknown')}`\n")
                        
                        if files and success > 0:
                            f.write(f"\n### Successfully Synced Articles\n")
                            for filepath in files[:10]:
                                f.write(f"- `{filepath}`\n")
                            if len(files) > 10:
                                f.write(f"- ... and {len(files) - 10} more\n")
                
                if failed > 0:
                    sys.exit(1)
        
        if __name__ == "__main__":
            syncer = DifyRAGSync()
            syncer.run()
        SCRIPT
    
    - name: Run synchronization
      env:
        DIFY_API_KEY: ${{ secrets.DIFY_API_KEY }}
        DIFY_API_URL: ${{ secrets.DIFY_API_URL }}
        DIFY_DATASET_ID: ${{ secrets.DIFY_DATASET_ID }}
        SYNC_ALL: ${{ github.event.inputs.sync_all || 'false' }}
      run: python sync_dify.py
