---
title: "【Amazon Bedrock】検索拡張⽣成(RAG) の精度を向上させるAdvanced RAGについて学んでみた"
emoji: "🌊"
type: "tech"
topics:
  - "aws"
  - "ai"
  - "bedrock"
  - "advancedrag"
  - "rag"
published: true
published_at: "2025-08-01 18:52"
---
# はじめに
本日AWS Partner向けのウェビナー「1 Day Solution Dive Deep : Advanced RAG on AWS Dive Deep」に参加し、Advanced RAGという手法を学びました。
Bedrockを使用する中で、どういった戦略でRAGの精度を向上できるのかをまとめていきたいと思います。

# Advanced RAGとは
基本的な RAG システム (Naive RAG / Baseline RAG) でも多くの場合で十分な性能を発揮しますが、より複雑な質問や高度な応用では、さらなる改善が必要となります。
`Advanced RAG` は、この基本的な RAG を様々な手法で拡張し、以下のような課題に対応するために開発されています。

https://aws.amazon.com/jp/blogs/news/a-practical-guide-to-improve-rag-systems-with-advanced-rag-on-aws/

![](https://d2908q01vomqb2.cloudfront.net/b3f0c7f6bb763af1be91d9e74eabfeb199dc1f1f/2024/09/19/advanced_rag_methods.png)

上記のように、手法は様々あり、`コスト`や`開発難易度`に差があります。
まずは、左から試してみるのが良さそうです。
また、それ以前の問題として、`プロンプトエンジニアリング`や`モデルの変更`などで解決できる場合もあります。

# 改善を考える前に評価ができるようにしておこう
RAGに限らず、改善案を実行する前に`改善前`と`改善後`で比較できる`評価`ができるようにしおく。

Amazon Bedrock Evaluationsを使用すれば、モデル評価が簡単に実装可能。
https://aws.amazon.com/jp/bedrock/evaluations/

# 基本的な RAG 改善テクニック

## チャンクサイズの調整
チャンクサイズはベクトル検索の基礎的な調整項目ですが、RAG システムの性能に大きな影響を与える重要な要素です。
小さすぎるチャンクは**検索の精度を上げる一方で、文脈の欠落を招く可能性**があります。
逆に、大きすぎるチャンクは**関連性の低い情報を含んでしまう可能性**があります。

### 固定チャギング
文書を、その中のコンテンツに関係なく固定サイズのチャンクに分割することです。
各チャンクには事前定義された数のトークンまたは文字が含まれ、この方法により、より均一なデータ組織が可能になります。
固定チャンキングは、チャンクが一貫したサイズであることを確保したい場合に有用で、予測可能な方法で処理と取得を容易にします。
文書は等しい長さのセクションに分割され、各セクションが別々のチャンクになります。
この方法は、コンテンツが比較的均質で、基礎となるコンテキストを理解するためにチャンク境界がそれほど重要でない場合にうまく機能します。

**メリット**
- **均一性**：各チャンクが同じサイズを持つため、システムがより予測可能になります。これは、各チャンクが一貫したサイズであることがわかるため、処理効率に役立ち、バッチ操作と並列処理を容易にします。
- **簡素化された取得**：チャンクサイズが均一であるため、データの検索が簡単になります。チャンクの長さを迅速に決定でき、大規模データセットでのパフォーマンス最適化とスケーラビリティに有用です。
- **パフォーマンス最適化**：固定チャンクは、文書取得とチャンキングの計算コストを制御したい場合に理想的です。等しいサイズのチャンクを持つことで、大規模文書処理を必要とするシナリオでの計算ボトルネックの可能性を減らします。

固定チャンキングは特定の使用例では効率的ですが、段落やセクションなどのコンテンツの自然な意味的境界を保持しない場合があります。
これにより、文やアイデアの途中で任意の場所で開始または終了するチャンクが生じ、コンテキストが切り取られる可能性があります。

### セマンティックチャギング
セマンティックチャンキングは、テキスト内の関係を（ベクトル埋め込みを使用して）分析し、埋め込みモデルによって計算されたセマンティック類似性に基づいてチャンクを作成します。
このアプローチは、取得時に情報の整合性を保持し、正確で文脈的に適切な結果を確保するのに役立ちます。

**メリット**
- テキストの意味とコンテキストに焦点を当てることで、セマンティックチャンキングは取得の品質を大幅に向上させます。テキストのセマンティック整合性を維持することが重要なシナリオで使用する必要があります。
- この方法は固定サイズのチャンキングよりも計算負荷が高いですが、文脈の境界が明確でない文書（例えば、法的文書、技術マニュアル、表が多すぎる文書など）のチャンキングに有益です。

固定チャギングより複雑な埋め込みを実施しているので、埋め込みモデルの呼び出し数が増え、固定チャギングよりコストがかかります。
>- セマンティックチャンキングプロセスはチャンク境界を見つけるためにベクトル埋め込みを生成するため、埋め込みLLMへのAPI呼び出しが増加します。したがって、固定サイズチャンキングよりも比較的高いコストが予想されます。
>- ただし、これらの操作の大部分は文書が最初に処理される時に発生します。定常状態では、これらのコストは新しい文書や変更された文書に対してのみ発生します。

### 階層的チャンク戦略
データを階層構造に整理し、データ内の固有の関係に基づいて、より詳細かつ効率的な取得を可能にします。
データを階層構造に整理することでRAGワークフローは、複雑にネストされたデータセットから効率的に情報をナビゲートし、取得できるようになります。
ドキュメントを解析した後、最初のステップは、**親**と**子のチャンクサイズ**に基づいてドキュメントを**チャンクに分割する**ことです。
- 親チャンク（上位レベル）は、ドキュメント全体やセクションなどのより大きなセグメントを表します。
- 子チャンク (下位レベル)は、段落や文などの小さなセグメントを表します。

**メリット**
- **効率的な検索セマンティック検索より大規模で包括的なコンテキスト**
    親と複数の⼦のネスト関係でチャンク化し検索⽤に**⼦**、応答⽣成⽤に**親**を利⽤します。
- **コンテキストの保持一貫性がありコンテキストに関連した**

**最適な使用例**
階層型チャンクは、次のようなネストされた階層構造を持つ**複雑なドキュメント**に最適です。
- **技術マニュアル**
- **法的文書**
- 複雑な書式とネストされた表を含む**学術論文**

## ドキュメントパースの改善
ドキュメントパースの改善は、特に PDF や複雑なフォーマットの文書を扱う際に重要です。
単純なテキスト抽出では、レイアウト情報や図表の内容が失われてしまう可能性があります。
マルチモーダルなモデルを使用して、図や表を Markdown などのテキスト形式に変換することができます。

## メタデータによるフィルタリング
メタデータを活用したフィルタリングは、検索結果の精度を大幅に向上させる効果的な手法です。
メタデータには、ドキュメントのタイトル、作成日時、著者、カテゴリなどが含まれます。

例えば、時系列データを扱う場合、ユーザーの質問に含まれる時間情報を元に、特定の期間のデータのみを検索対象とすることができます。
これにより、不適切な時期の情報が回答に混入することを防ぎ、より正確な回答を生成することができます。

## ハイブリッド検索
![](https://d2908q01vomqb2.cloudfront.net/b3f0c7f6bb763af1be91d9e74eabfeb199dc1f1f/2024/09/19/hybrid_search.png)

**ハイブリッド検索**とは、
`キーワード検索`による明確さ・正確性と、`ベクトル検索`の意味的な類似度比較の強みを併せ持つ検索手法であるため、より高い精度で文書を検索することができます。
キーワード検索やベクトル検索の処理速度や検索精度のデメリットを解消しつつ、メリットを更に強く発揮できるのがこのハイブリッドです。

## リランキング
>リランキングは、初期の検索結果をさらに精緻化する手法です。通常の検索エンジンが返す結果の順序を、より高度なモデルを使用して再評価し、最も関連性の高い情報を上位に配置します。
この手法の利点は、初期検索の広範な結果を維持しつつ、最終的な順序をより洗練させることができる点です。
例えば、ある質問に対して検索結果 A, B, C, D が得られたとすると、リランキングモデルはこれらの結果とクエリとの関連性を詳細に分析し、最も適切な順序に並べ替えます。
リランキングモデルには、Cohere が提供する多言語対応のモデルが広く使用されています。
このモデルは、Amazon SageMaker JumpStart を通じて簡単にデプロイし、利用することができます。
詳しくはブログ記事 “Improve RAG performance using Cohere Rerank” を参照してください。また、オープンソースのモデルを自前でホストしたり、LLM を使用してリランキングを行うこともできます。

イメージとしては、ユーザからの入力を埋め込みモデルでベクトル変換後、リランクモデルを使用して、ベクトル検索でドキュメントのリスト取得・ランク付けを実施します。
ランク付けされた上位コンテンツをもとに基盤モデルで回答生成後、最終的な回答をユーザへ返す流れです。


![](https://d2908q01vomqb2.cloudfront.net/b3f0c7f6bb763af1be91d9e74eabfeb199dc1f1f/2024/09/19/reranking.png)

## クエリ書き換え
>クエリ書き換えは、ユーザーの入力した質問を、検索システムにとってより最適な形に変換する技術です。この手法は、ユーザーの意図をより正確に捉え、関連性の高い情報を効果的に検索するのに役立ちます。クエリ書き換えには、主に以下のようなアプローチがあります。

- **クエリの簡略化**: 複雑な質問を核心的なキーワードに絞り込む。例えば、「RAG を使った検索で、検索の精度を上げるための方法をいくつか教えてください」というクエリを「RAG の精度向上」に簡略化する。
- **クエリの拡張**: 元のクエリに関連する用語や同義語を追加し、検索範囲を広げる。例えば、「大規模言語モデル」というキーワードに対して「Large Language Model」、「LLM」、「生成 AI」といった関連用語を追加する。
- **クエリの分解**: 複雑な質問を複数の簡単な質問に分解する。例えば、「クリーンエネルギーの世界的な普及における主要な障害と、それらを克服するための国際的な努力は何か？」という質問を、「クリーンエネルギーの定義は何か」「クリーンエネルギー普及の主な障害は何か」「どの国際組織がクリーンエネルギー普及を推進しているか」といった複数の質問に分解する。

## Graph RAG
GraphRAG は、従来のベクトルデータベースの代わりに、ナレッジグラフを使用してドキュメントの知識を表現する手法です。
ナレッジグラフは、情報をノードとエッジの形で構造化して表現し、データ間の関係性をより明確に捉えることができます。GraphRAG の主な利点は以下の通りです。

- 複雑な関係性の表現: ナレッジグラフを用いることで、文書間や概念間の複雑な関係性を明示的に表現できる。これにより、単純なキーワードマッチングやベクトル類似度では捉えきれない情報の関連性を考慮した検索が可能になる。
- 多段階の推論: グラフ構造を活用することで、直接的な関連がない情報同士をつなぎ合わせる多段階の推論が可能になる。これは、複数の文書にまたがる情報を必要とする複雑な質問に特に有効となる。
- 説明可能性の向上: 回答生成の過程で使用された情報の関係性を、グラフ上のパスとして視覚化できるため、回答の根拠をより明確に示すことができる。
コンテキストの保持: グラフ構造によって情報の階層性や文脈を保持できるため、より広い文脈を考慮した回答生成が可能になる。

## 実務に活用できる参考サイト
https://aws.amazon.com/jp/builders-flash/202502/way-to-succeed-rag-project/
https://aws.amazon.com/jp/builders-flash/202504/way-to-succeed-rag-project-2/
https://docs.aws.amazon.com/ja_jp/prescriptive-guidance/latest/writing-best-practices-rag/introduction.html

## おわりに
生成AI界隈は日々新しい技術が増えていき、解決手法も様々な種類がでています。
自分たちが達成したい課題やコスト、開発期間などをしっかりと見極めて適切な手法をとっていくことが重要と感じます。
